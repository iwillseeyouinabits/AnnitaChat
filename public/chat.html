<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Voice Assistant</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, sans-serif;
      max-width: 600px;
      margin: 0 auto;
      padding: 20px;
      background-color: #f9f9f9;
      color: #333;
      display: flex;
      justify-content: center;
      align-items: center;
      min-height: 100vh;
    }
    
    .app-container {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 20px;
      width: 100%;
    }
    
    .back-button {
      background-color: transparent;
      color: #666;
      border: 1px solid #ddd;
      border-radius: 8px;
      padding: 8px 16px;
      font-size: 14px;
      cursor: pointer;
      margin-bottom: 20px;
      display: flex;
      align-items: center;
      gap: 8px;
      transition: background-color 0.3s;
      align-self: flex-start;
    }
    
    .back-button:hover {
      background-color: #f0f0f0;
    }
    
    .mic-container {
      position: relative;
      margin: 30px 0;
    }
    
    .mic-button {
      display: flex;
      align-items: center;
      justify-content: center;
      width: 120px;
      height: 120px;
      border-radius: 50%;
      background-color: #4285f4;
      color: white;
      font-size: 48px;
      cursor: pointer;
      border: none;
      box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2);
      transition: all 0.3s;
      margin: 0 auto;
    }
    
    .mic-button:hover {
      transform: scale(1.05);
      box-shadow: 0 8px 16px rgba(0, 0, 0, 0.3);
    }
    
    .mic-button:disabled {
      background-color: #ccc;
      cursor: not-allowed;
      transform: none;
      box-shadow: none;
    }
    
    .mic-button.recording {
      animation: pulse 1.5s infinite;
      background-color: #34a853; /* Green when recording */
    }
    
    .mic-button.processing {
      background-color: #fbbc05; /* Yellow when processing */
    }
    
    .mic-button.speaking {
      background-color: #ea4335; /* Red when speaking */
    }
    
    @keyframes pulse {
      0% { transform: scale(1); }
      50% { transform: scale(1.05); opacity: 0.9; }
      100% { transform: scale(1); }
    }
    
    .status-dot {
      position: absolute;
      width: 16px;
      height: 16px;
      border-radius: 50%;
      background-color: #ccc;
      bottom: -5px;
      left: 50%;
      transform: translateX(-50%);
    }
    
    .status-dot.connected {
      background-color: #34a853;
    }
    
    .status-dot.speaking {
      background-color: #fbbc05;
      animation: pulse 1.5s infinite;
    }
    
    .status-info {
      font-size: 16px;
      color: #666;
      text-align: center;
      margin-top: 20px;
      min-height: 20px;
    }
    
    .connecting-overlay {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background-color: rgba(0, 0, 0, 0.5);
      display: flex;
      justify-content: center;
      align-items: center;
      z-index: 1000;
      opacity: 0;
      pointer-events: none;
      transition: opacity 0.3s;
    }
    
    .connecting-overlay.visible {
      opacity: 1;
      pointer-events: all;
    }
    
    .connecting-box {
      background-color: white;
      padding: 20px 40px;
      border-radius: 8px;
      text-align: center;
    }
    
    .connecting-spinner {
      border: 4px solid #f3f3f3;
      border-top: 4px solid #4285f4;
      border-radius: 50%;
      width: 30px;
      height: 30px;
      animation: spin 1s linear infinite;
      margin: 10px auto;
    }
    
    @keyframes spin {
      0% { transform: rotate(0deg); }
      100% { transform: rotate(360deg); }
    }
  </style>
</head>
<body>
  <div class="app-container">
    <button id="backButton" class="back-button">
      <span>‚Üê</span> Back to Settings
    </button>
    
    <div class="mic-container">
      <button id="micBtn" class="mic-button">üé§</button>
      <div id="statusDot" class="status-dot"></div>
    </div>
    
    <div id="statusInfo" class="status-info">Click the microphone to start</div>
  </div>
  
  <div class="connecting-overlay" id="connectingOverlay">
    <div class="connecting-box">
      <div class="connecting-spinner"></div>
      <p>Connecting...</p>
    </div>
  </div>

  <script src="https://cdn.socket.io/4.5.4/socket.io.min.js"></script>
  <script>
    // Parse URL parameters
    const urlParams = new URLSearchParams(window.location.search);
    const roleParam = urlParams.get('role');
    const voiceParam = urlParams.get('voice');
    const noiseParam = urlParams.get('noise');
    
    // Decode parameters
    const assistantRole = roleParam ? decodeURIComponent(roleParam) : '';
    const voiceId = voiceParam ? decodeURIComponent(voiceParam) : '21m00Tcm4TlvDq8ikWAM'; // Default to Rachel
    const noiseThreshold = noiseParam ? parseInt(noiseParam) : -45; // Default noise threshold
    const silenceThreshold = 1500; // Default 1.5 seconds for silence detection
    
    console.log('Settings received:', { 
      role: assistantRole, 
      voice: voiceId, 
      noiseThreshold: noiseThreshold + ' dB'
    });
    
    // Audio processing variables
    let audioContext;
    let analyser;
    let microphone;
    let scriptProcessor;
    let isSpeechDetected = false;
    let silenceStart = null;
    
    // DOM Elements
    const backButton = document.getElementById('backButton');
    const micBtn = document.getElementById('micBtn');
    const statusDot = document.getElementById('statusDot');
    const statusInfo = document.getElementById('statusInfo');
    const connectingOverlay = document.getElementById('connectingOverlay');
    
    // State variables
    let socket;
    let recognition;
    let audioPlayer = new Audio();
    let isConnected = false;
    let isRecording = false;
    let isSpeaking = false;
    let isProcessing = false;
    let finalTranscript = '';
    let silenceTimer = null;
    let recognitionRestartTimer = null;
    let conversationHistory = [];
    const MAX_HISTORY_LENGTH = 100;
    
    // Navigation back to settings
    backButton.addEventListener('click', () => {
      // Disconnect if connected
      if (isConnected) {
        disconnectSocket();
      }
      
      // Go back to settings page
      window.location.href = 'index.html';
    });
    
    // Update the visual state to match the current status
    function updateVisualState() {
      // Remove all state classes first
      micBtn.classList.remove('recording', 'speaking', 'processing');
      
      // Update emoji based on state
      if (isSpeaking) {
        micBtn.textContent = 'üó£Ô∏è'; // Speaking emoji
        micBtn.classList.add('speaking');
      } else if (isProcessing) {
        micBtn.textContent = 'üí≠'; // Thinking emoji
        micBtn.classList.add('processing');
      } else if (isRecording) {
        micBtn.textContent = 'üé§'; // Microphone emoji
        micBtn.classList.add('recording');
      } else {
        micBtn.textContent = 'üé§'; // Default microphone emoji
      }
    }
    
    // Audio player setup
    audioPlayer.onplay = () => {
      isSpeaking = true;
      statusDot.classList.add('speaking');
      statusInfo.textContent = "AI is speaking";
      
      // Completely stop speech recognition while AI is speaking
      if (recognition) {
        try {
          recognition.stop();
          isRecording = false;
        } catch (e) {
          console.log("Error stopping recognition:", e);
        }
      }
      
      // Pause audio processing to prevent picking up AI's voice
      pauseAudioProcessing();
      
      // Update visual state
      updateVisualState();
    };
    
    audioPlayer.onpause = audioPlayer.onended = () => {
      isSpeaking = false;
      statusDot.classList.remove('speaking');
      
      if (isConnected) {
        statusInfo.textContent = "Listening...";
        
        // Resume audio processing after AI finishes speaking
        resumeAudioProcessing();
        
        // Start recognition again after AI finishes speaking if we're connected
        if (!isRecording) {
          startRecognition();
        }
      }
      
      // Update visual state
      updateVisualState();
    };
    
    // Initialize speech recognition
    function initSpeechRecognition() {
      if ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = 'en-US';
        
        recognition.onstart = () => {
          isRecording = true;
          updateVisualState();
          statusInfo.textContent = "Listening...";
          
          // Clear any pending send timeout
          if (silenceTimer) {
            clearTimeout(silenceTimer);
          }
          
          // Clear any recognition restart timer
          if (recognitionRestartTimer) {
            clearTimeout(recognitionRestartTimer);
            recognitionRestartTimer = null;
          }
        };
        
        recognition.onend = () => {
          console.log("Recognition ended");
          isRecording = false;
          updateVisualState();
          
          // If still connected and not speaking, restart recognition after a brief pause
          if (isConnected && !isSpeaking) {
            statusInfo.textContent = "Restarting listening...";
            
            // Only restart if we're not deliberately disconnecting
            recognitionRestartTimer = setTimeout(() => {
              if (isConnected && !isSpeaking) {
                startRecognition();
              }
            }, 300);
          } else if (!isConnected) {
            statusInfo.textContent = "Click the microphone to start";
          }
        };
        
        recognition.onresult = (event) => {
          let interimTranscript = '';
          let hadNewFinalResult = false;
          
          for (let i = event.resultIndex; i < event.results.length; i++) {
            const transcript = event.results[i][0].transcript;
            
            if (event.results[i].isFinal) {
              finalTranscript += transcript + ' ';
              hadNewFinalResult = true;
            } else {
              interimTranscript += transcript;
            }
          }
          
          // Display current speech in status info
          if (interimTranscript) {
            statusInfo.textContent = interimTranscript;
          } else if (finalTranscript) {
            statusInfo.textContent = finalTranscript;
          } else {
            statusInfo.textContent = "Listening...";
          }
        };
        
        recognition.onerror = (event) => {
          console.error('Speech recognition error:', event.error);
          
          // Don't stop recording for common errors like "no-speech"
          if (event.error === 'no-speech') {
            console.log("No speech detected, continuing to listen");
            return;
          }
          
          isRecording = false;
          updateVisualState();
          
          // For network errors, we might need to reconnect
          if (event.error === 'network') {
            statusInfo.textContent = "Network error. Reconnecting...";
            handleDisconnection();
            
            // Try to reconnect after a brief delay
            setTimeout(connectSocket, 2000);
          } else {
            // For other errors, try to restart recognition if still connected
            if (isConnected && !isSpeaking) {
              statusInfo.textContent = `Error: ${event.error}. Restarting...`;
              
              recognitionRestartTimer = setTimeout(() => {
                if (isConnected && !isSpeaking) {
                  startRecognition();
                }
              }, 1000);
            }
          }
        };
      } else {
        alert('Speech recognition is not supported in your browser.');
        micBtn.disabled = true;
        statusInfo.textContent = "Speech recognition not supported";
      }
    }
    
    // Helper function to start recognition
    function startRecognition() {
      if (!recognition || !isConnected || isSpeaking) return;
      
      try {
        finalTranscript = '';
        recognition.start();
        console.log("Recognition started");
      } catch (e) {
        console.log('Recognition error:', e);
        
        // If already started, stop and restart after a delay
        if (e.message.includes('already started')) {
          recognition.stop();
          setTimeout(() => {
            if (isConnected && !isSpeaking) {
              recognition.start();
            }
          }, 100);
        }
      }
    }
    
    // Initialize speech recognition if browser supports it
    if ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {
      initSpeechRecognition();
    } else {
      micBtn.disabled = true;
      statusInfo.textContent = "Speech recognition not supported";
      alert('Speech recognition is not supported in your browser.');
    }
    
    // Mic button behavior
    micBtn.addEventListener('click', () => {
      if (!isConnected && !isRecording) {
        // First click: Connect and start listening
        statusInfo.textContent = "Connecting...";
        connectSocket();
      } else if (isConnected) {
        // If connected: Disconnect
        statusInfo.textContent = "Disconnecting...";
        disconnectSocket();
      }
    });
    
    function connectSocket() {
      showConnectingOverlay(true);
      
      // Clear any existing connection
      if (socket) {
        socket.disconnect();
      }
      
      socket = io();
      
      socket.on('connect', () => {
        isConnected = true;
        statusDot.classList.add('connected');
        showConnectingOverlay(false);
        
        statusInfo.textContent = "Connected. Starting listening...";
        
        // Initialize audio processing
        initAudioProcessing().then(() => {
          // Start the conversation with an AI greeting
          sendInitialGreeting();
          
          // Start speech recognition after a delay to allow the greeting to complete
          setTimeout(() => {
            if (isConnected && !isSpeaking) {
              startRecognition();
            }
          }, 500);
        }).catch(error => {
          console.error('Error initializing audio:', error);
          statusInfo.textContent = "Error accessing microphone";
        });
      });
      
      socket.on('disconnect', () => {
        handleDisconnection();
      });
      
      socket.on('chat_response', (data) => {
        const responseText = data.choices[0].message.content;
        
        // Add the AI response to the conversation history
        conversationHistory.push({ role: 'assistant', content: responseText });
        
        // Speak the response
        speakText(responseText);
      });
      
      let streamedResponse = '';
      socket.on('chat_response_chunk', (data) => {
        streamedResponse += data.content;
      });
      
      socket.on('chat_response_done', () => {
        // Add the complete streamed response to conversation history
        if (streamedResponse.trim()) {
          conversationHistory.push({ role: 'assistant', content: streamedResponse });
        }
        
        // Speak the response when streaming is complete
        speakText(streamedResponse);
        streamedResponse = '';
      });
      
      socket.on('chat_error', (error) => {
        statusInfo.textContent = `Server error: ${error.message}`;
        isProcessing = false;
        updateVisualState();
      });
      
      socket.on('connect_error', () => {
        showConnectingOverlay(false);
        statusInfo.textContent = "Connection error";
        handleDisconnection();
      });
    }
    
    // Initialize audio context and set up audio processing for noise detection
    async function initAudioProcessing() {
      try {
        // Create audio context
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        analyser = audioContext.createAnalyser();
        
        // Set up analyser node
        analyser.minDecibels = -90;
        analyser.maxDecibels = -10;
        analyser.smoothingTimeConstant = 0.85;
        analyser.fftSize = 2048;
        
        // Get microphone input
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        microphone = audioContext.createMediaStreamSource(stream);
        microphone.connect(analyser);
        
        // Create script processor for analysis
        scriptProcessor = audioContext.createScriptProcessor(2048, 1, 1);
        analyser.connect(scriptProcessor);
        scriptProcessor.connect(audioContext.destination);
        
        // Process audio data
        scriptProcessor.addEventListener('audioprocess', analyzeAudio);
        
        return true;
      } catch (error) {
        console.error('Error setting up audio processing:', error);
        throw error;
      }
    }
    
    // Pause audio processing (disconnect the script processor)
    function pauseAudioProcessing() {
      if (scriptProcessor) {
        scriptProcessor.disconnect();
      }
    }
    
    // Resume audio processing (reconnect the script processor)
    function resumeAudioProcessing() {
      if (scriptProcessor && analyser && audioContext) {
        scriptProcessor.connect(audioContext.destination);
        analyser.connect(scriptProcessor);
      }
    }
    
    // Clean up audio processing
    function cleanupAudioProcessing() {
      if (scriptProcessor) {
        scriptProcessor.removeEventListener('audioprocess', analyzeAudio);
        scriptProcessor.disconnect();
        scriptProcessor = null;
      }
      
      if (analyser) {
        analyser.disconnect();
        analyser = null;
      }
      
      if (microphone) {
        microphone.disconnect();
        microphone = null;
      }
      
      if (audioContext && audioContext.state !== 'closed') {
        audioContext.close();
        audioContext = null;
      }
    }
    
    // Analyze audio for volume levels
    function analyzeAudio(event) {
      if (!analyser || !isConnected || isSpeaking) return;
      
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);
      analyser.getByteFrequencyData(dataArray);
      
      // Calculate average volume
      let sum = 0;
      for (let i = 0; i < bufferLength; i++) {
        sum += dataArray[i];
      }
      const average = sum / bufferLength;
      
      // Convert to decibels (approximate)
      const volumeDb = average === 0 ? -90 : 20 * Math.log10(average / 255);
      
      // Check if volume is above threshold
      const isAboveThreshold = volumeDb > noiseThreshold;
      
      // Handle speech detection state
      if (isAboveThreshold && !isSpeechDetected) {
        // Speech started
        isSpeechDetected = true;
        silenceStart = null;
        
        // Optional: show volume level in status (for debugging)
        // statusInfo.textContent = `Volume: ${volumeDb.toFixed(1)} dB`;
      } 
      else if (!isAboveThreshold && isSpeechDetected) {
        // Potential end of speech, start silence timer
        if (!silenceStart) {
          silenceStart = Date.now();
        } 
        else if (Date.now() - silenceStart > silenceThreshold) {
          // Silence duration exceeded threshold, trigger end of speech
          isSpeechDetected = false;
          silenceStart = null;
          
          // If we have transcribed text, send it after silence
          if (finalTranscript.trim()) {
            sendMessage(finalTranscript);
            finalTranscript = '';
          }
        }
      } 
      else if (!isAboveThreshold) {
        // Still below threshold, reset speech detection
        isSpeechDetected = false;
        silenceStart = null;
      }
    }
    
    function disconnectSocket() {
      // Clear any recognition restart timer
      if (recognitionRestartTimer) {
        clearTimeout(recognitionRestartTimer);
        recognitionRestartTimer = null;
      }
      
      // Stop recognition first
      if (recognition) {
        try {
          recognition.stop();
        } catch (e) {
          console.log("Error stopping recognition:", e);
        }
      }
      
      // Clean up audio processing
      cleanupAudioProcessing();
      
      // Then disconnect socket
      if (socket) {
        socket.disconnect();
      }
      
      handleDisconnection();
    }
    
    // Send an initial greeting from the AI to start the conversation
    function sendInitialGreeting() {
      if (!socket) return;
      
      // Set processing state
      isProcessing = true;
      updateVisualState();
      statusInfo.textContent = "AI is starting the conversation...";
      
      // Create messages array with system message if provided
      let messages = [];
      if (assistantRole) {
        messages.push({ role: 'system', content: assistantRole });
      }
      
      // Add a system message instructing the AI to greet the user
      messages.push({ 
        role: 'system', 
        content: 'The user has just connected. Greet them with a short, friendly welcome and briefly explain what you can help with. Keep it concise.' 
      });
      
      const chatRequest = {
        messages: messages,
        model: 'gpt-3.5-turbo',
        stream: true
      };
      
      socket.emit('chat', chatRequest);
    }
    
    function handleDisconnection() {
      isConnected = false;
      isProcessing = false;
      statusDot.classList.remove('connected');
      statusDot.classList.remove('speaking');
      statusInfo.textContent = "Click the microphone to start";
      
      // Stop any ongoing speech
      if (isSpeaking) {
        audioPlayer.pause();
        audioPlayer.currentTime = 0;
      }
      
      // Clear any timers
      if (silenceTimer) {
        clearTimeout(silenceTimer);
        silenceTimer = null;
      }
      
      // Clear any recognition restart timer
      if (recognitionRestartTimer) {
        clearTimeout(recognitionRestartTimer);
        recognitionRestartTimer = null;
      }
      
      // Reset audio processing state
      isSpeechDetected = false;
      silenceStart = null;
      
      // Update visual state
      updateVisualState();
    }
    
    function showConnectingOverlay(show) {
      if (show) {
        connectingOverlay.classList.add('visible');
      } else {
        connectingOverlay.classList.remove('visible');
      }
    }
    
    function sendMessage(message) {
      message = message.trim();
      if (!message || !socket) return;
      
      // Set processing state
      isProcessing = true;
      updateVisualState();
      statusInfo.textContent = "Processing...";
      
      // Create a new messages array starting with the system message if provided
      let messages = [];
      
      // Add system message if role is provided
      if (assistantRole) {
        messages.push({ role: 'system', content: assistantRole });
      }
      
      // Add the user's new message to the conversation history
      conversationHistory.push({ role: 'user', content: message });
      
      // Limit history to prevent excessive tokens
      if (conversationHistory.length > MAX_HISTORY_LENGTH) {
        // Keep the most recent messages
        conversationHistory = conversationHistory.slice(-MAX_HISTORY_LENGTH);
      }
      
      // Include all conversation history in the request
      messages = messages.concat(conversationHistory);
      
      const chatRequest = {
        messages: messages,
        model: 'gpt-3.5-turbo',
        stream: true
      };
      
      socket.emit('chat', chatRequest);
    }
    
    function speakText(text) {
      // Don't speak if text is empty or just whitespace
      if (!text || !text.trim()) {
        isProcessing = false;
        updateVisualState();
        return;
      }
      
      // If currently playing, stop it
      if (isSpeaking) {
        audioPlayer.pause();
        audioPlayer.currentTime = 0;
      }
      
      statusInfo.textContent = "Generating speech...";
      
      // Use Eleven Labs API for speech with the voice ID from settings
      fetch('/api/elevenlabs/text-to-speech', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ 
          text,
          voiceId
        }),
      })
      .then(response => {
        if (!response.ok) {
          throw new Error('Failed to generate speech');
        }
        return response.blob();
      })
      .then(audioBlob => {
        const audioUrl = URL.createObjectURL(audioBlob);
        audioPlayer.src = audioUrl;
        isProcessing = false;
        updateVisualState();
        audioPlayer.play();
      })
      .catch(error => {
        console.error('Failed to get Eleven Labs audio:', error);
        statusInfo.textContent = "Error generating speech";
        isProcessing = false;
        updateVisualState();
        
        // Start recognition again if it's not running
        if (isConnected && !isRecording && !isSpeaking) {
          startRecognition();
        }
      });
    }
    
    // Handle page visibility changes
    document.addEventListener('visibilitychange', () => {
      if (document.visibilityState === 'hidden') {
        // Page is hidden, pause everything but don't disconnect
        if (isRecording && recognition) {
          try {
            recognition.stop();
          } catch (e) {
            console.log("Error stopping recognition:", e);
          }
        }
        
        if (isSpeaking) {
          audioPlayer.pause();
        }
      } else if (document.visibilityState === 'visible' && isConnected) {
        // Page is visible again, resume activities if connected
        if (isSpeaking) {
          audioPlayer.play();
        } else if (!isRecording && !isSpeaking) {
          // Restart recognition if not already recording and not speaking
          startRecognition();
        }
      }
    });
  </script>
</body>
</html>